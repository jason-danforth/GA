{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://imgur.com/1ZcRyrc.png\" style=\"float: left; margin: 20px; height: 55px\">\n",
    "\n",
    "# Train/Test Split and Bias and Variance\n",
    "\n",
    "_Authors: Joseph Nelson (DC), Kevin Markham (DC)_\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "<a id=\"learning-objectives\"></a>\n",
    "### Learning Objectives\n",
    "- Define error due to bias and error due to variance.\n",
    "- Identify the bias-variance trade-off.\n",
    "- Describe what overfitting and underfitting means in the context of model building.\n",
    "- Explain problems associated with over- and underfitting.\n",
    "- Grasp why train/test split is necessary.\n",
    "- Explore k-folds, LOOCV, and three split methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Lesson Guide\n",
    "- [Bias and Variance Trade-Off](#bias-and-variance-trade-off)\n",
    "\t- [Bias? Variance?](#bias-variance)\n",
    "\t- [Exploring the Bias-Variance Trade-Off](#exploring-the-bias-variance-tradeoff)\n",
    "\t- [Brain and Body Weight Mammal Data Set](#brain-and-body-weight-mammal-dataset)\n",
    "\t- [Making a Prediction](#making-a-prediction)\n",
    "- [Making a Prediction From a Sample](#making-a-prediction-from-a-sample)\n",
    "\t- [Let's Try Something Completely Different](#lets-try-something-completely-different)\n",
    "- [Balancing Bias and Variance](#balancing-bias-and-variance)\n",
    "- [Train/Test Split](#train-test-split)\n",
    "\t- [Evaluation Procedure #1: Train and Test on the Entire Data Set (Do Not Do This)](#evaluation-procedure--train-and-test-on-the-entire-dataset-do-not-do-this)\n",
    "\t- [Problems With Training and Testing on the Same Data](#problems-with-training-and-testing-on-the-same-data)\n",
    "\t- [Evaluation Procedure #2: Train/Test Split](#evaluation-procedure--traintest-split)\n",
    "\t- [Comparing Test Performance With a Null Baseline](#comparing-test-performance-with-a-null-baseline)\n",
    "- [K-Folds Cross-Validation](#k-folds-cross-validation)\n",
    "\t- [Leave-One-Out Cross-Validation](#leave-one-out-cross-validation)\n",
    "\t- [Intro to Cross-Validation With the Boston Data](#intro-to-cross-validation-with-the-boston-data)\n",
    "- [Three-Way Data Split](#three-way-data-split)\n",
    "\t- [Additional Resources](#additional-resources)\n",
    "- [Summary](#summary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-and-variance-trade-off\"></a>\n",
    "## Bias and Variance Trade-Off\n",
    "---\n",
    "\n",
    "The **bias-variance tradeoff** is widely used in machine learning as a conceptual way of comparing and contrasting different models. We only have a few methods that are able to compare all machine learning models. The others are more mathematical.\n",
    "\n",
    "**Bias** is error stemming from incorrect model assumptions.\n",
    "- Example: Assuming data is linear when it has a more complicated structure.\n",
    "\n",
    "**Variance** is error stemming from being overly sensitive from changes to the training data.\n",
    "- Example: Using the training set exactly (e.g. 1-NN) for a model results in a completely different model -- even if the training set differs only slightly.\n",
    "\n",
    "\n",
    "As model complexity **increases**:\n",
    "- Bias **decreases**. (The model can more accurately model complex structure in data.)\n",
    "- Variance **increases**. (The model identifies more complex structures, making it more sensitive to small changes in the training data.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"bias-variance\"></a>\n",
    "### Bias? Variance?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Conceptual Definitions**\n",
    "- **Bias**: How close are predictions to the actual values?\n",
    "  - Roughly, whether or not our model aims on target.\n",
    "  - If the model cannot represent the data's structure, our predictions could be consistent, but will not be accurate.\n",
    "- **Variance**: How variable are our predictions?\n",
    "  - Roughly, whether or not our model is reliable.\n",
    "  - We will make slightly different predictions given slightly different training sets."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/biasVsVarianceImage.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Visually, we are building a model where the bulls-eye is the goal.\n",
    "- Each individual hit is one prediction based on our model.\n",
    "- Critically, the success of our model (low variance, low bias) depends on the training data present."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Examples:**\n",
    "\n",
    "- **Linear regression:** Low variance, High bias.\n",
    "    - If we train with a different subset of the training set, the model will be about the same. Hence, the model has low variance.\n",
    "    - The resulting model will predict the training points incorrectly (unless they happen to be perfectly linear). Hence, it has high bias.\n",
    "   \n",
    "\n",
    "- **Nearest neighbor:** High variance, Low bias.\n",
    "    - If we train with a different subset of the training set, the model will make predictions very differently. Hence, the model is highly variable.\n",
    "    - The resulting model will predict every training point perfectly. Hence, it has low bias.\n",
    "\n",
    "- **K-Nearest neightbor:** Med-high variance, Med-low bias.\n",
    "    - The model itself is more robust to outliers, so it will make more predictions the same than before. Hence, it has lower variance than 1-NN.\n",
    "    - The resulting model no longer predicts every point perfectly, since outliers will be mispredicted. So, the bias will be higher than before.\n",
    "\n",
    "\n",
    "See if you can figure out:\n",
    "\n",
    "- **High-order polynomial (as compared to linear regression)**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expressing bias and variance mathematically:**\n",
    "\n",
    "It can be helpful understanding these terms by looking at how we can decompose the total error into them mathematically. (We will skip the derivations for now!)\n",
    "\n",
    "Let's define the error of our predictor as the expected value of our squared error. Note this error is not based on any particular fitted model, but on the family of potential models given a dataset (i.e. all fitted models made from all possible subsets of data).\n",
    "\n",
    "$$E[(y - \\hat{f}(x))^2] = Bias[\\hat{f}(x)]^2 + Var[\\hat{f}(x)] + \\sigma^2$$\n",
    "\n",
    "This states the expected error is based on only three components: **bias**, **variance**, and **irreducible error**.\n",
    "\n",
    "Breaking the bias and variance down further:\n",
    "\n",
    "$$Bias[\\hat{f}(x)] = E[\\hat{f}(x) - f(x)].$$\n",
    "\n",
    "- The bias is just the average expected distance between our predictor and actual values.\n",
    "\n",
    "$$Var[\\hat{f}(x)] = E[\\hat{f}(x)^2] - E[\\hat{f}(x)]^2.$$\n",
    "\n",
    "- The variance is how much our predictions vary about the mean. ($E[\\hat{f}(x)]$ is our predictor's mean prediction.)\n",
    "\n",
    "- The irreducible error stems from noise in the problem itself.\n",
    "\n",
    "**Some common questions:**\n",
    "\n",
    "From the math above, we can answer a few common questions:\n",
    "\n",
    "Can a model have high bias given one dataset and low bias for another?\n",
    "- Yes. If our data is linearly related, for example, it will have low bias on a linear model! However, in general across all datasets very few are accurately described with a linear model. So, in general we say a linear model has high bias and low variance.\n",
    "\n",
    "Is the MSE for a fitted linear regression the same thing as the bias?\n",
    "- It's close, but bias does not apply to a specific fitted model. Bias is the expected error of a model no matter what subset of the data it is fit on. This way, if we happen to get a lucky MSE fitting a model on a particular subset of our data, this does not mean we will have a low bias overall."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"exploring-the-bias-variance-tradeoff\"></a>\n",
    "### Exploring the Bias-Variance Trade-Off"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Allow plots to appear in the notebook.\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"brain-and-body-weight-mammal-dataset\"></a>\n",
    "### Brain and Body Weight Mammal Data Set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is a [data set](http://people.sc.fsu.edu/~jburkardt/datasets/regression/x01.txt) of the average weight of the body (in kg) and the brain (in g) for 62 mammal species. We'll use this dataset to investigate bias vs. variance. Let's read it into Pandas and take a quick look:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brain</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>3.385</td>\n",
       "      <td>44.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.480</td>\n",
       "      <td>15.5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.350</td>\n",
       "      <td>8.1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>465.000</td>\n",
       "      <td>423.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>36.330</td>\n",
       "      <td>119.5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     brain   body\n",
       "0    3.385   44.5\n",
       "1    0.480   15.5\n",
       "2    1.350    8.1\n",
       "3  465.000  423.0\n",
       "4   36.330  119.5"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "path = 'data/mammals.txt'\n",
    "cols = ['brain','body']\n",
    "mammals = pd.read_table(path, sep='\\t', names=cols, header=0)\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>brain</th>\n",
       "      <th>body</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>62.000000</td>\n",
       "      <td>62.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>198.789984</td>\n",
       "      <td>283.134194</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>899.158011</td>\n",
       "      <td>930.278942</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.005000</td>\n",
       "      <td>0.140000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.600000</td>\n",
       "      <td>4.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>3.342500</td>\n",
       "      <td>17.250000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>48.202500</td>\n",
       "      <td>166.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>6654.000000</td>\n",
       "      <td>5712.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "             brain         body\n",
       "count    62.000000    62.000000\n",
       "mean    198.789984   283.134194\n",
       "std     899.158011   930.278942\n",
       "min       0.005000     0.140000\n",
       "25%       0.600000     4.250000\n",
       "50%       3.342500    17.250000\n",
       "75%      48.202500   166.000000\n",
       "max    6654.000000  5712.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mammals.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We're going to focus on a smaller subset in which the body weight is less than 200 kg."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Only keep rows in which the body weight is less than 200 kg.\n",
    "mammals = mammals[mammals.body < 200]\n",
    "mammals.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We're now going to pretend that there are only 51 mammal species in existence. In other words, we are pretending that this is the entire data set of brain and body weights for **every known mammal species**.\n",
    "\n",
    "Let's create a scatterplot (using [Seaborn](http://stanford.edu/~mwaskom/software/seaborn/)) to visualize the relationship between brain and body weight:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, fit_reg=False);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "There appears to be a relationship between brain and body weight for mammals."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"making-a-prediction\"></a>\n",
    "### Making a Prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"linear-regression-quick-review\"></a>\n",
    "#### Linear Regression: A Quick Review\n",
    "\n",
    "![](./assets/linear-residuals.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Now let's pretend that a **new mammal species** is discovered. We measure the body weight of every member of this species we can find and calculate an **average body weight of 100 kgs**. We want to **predict the average brain weight** of this species (rather than measuring it directly). How might we do this?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We drew a straight line that appears to best capture the relationship between brain and body weight. So, we might predict that our new species has a brain weight of about 45 g, as that's the approximate y value when x=100.\n",
    "\n",
    "This is known as a \"linear model\" or a \"linear regression model.\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"making-a-prediction-from-a-sample\"></a>\n",
    "## Making a Prediction From a Sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Earlier, we assumed that this dataset contained every known mammal species. That's very convenient, but **in the real world, all you ever have is a sample of data**. This may sound like a contentious statement, but the point of machine learning is to generalize from a sample to the population. If you already have data for the entire population, then you have no need for machine learning -- you can apply statistics directly and get optimal answers!\n",
    "\n",
    "Here, a more realistic situation would be to only have brain and body weights for (let's say) half of the 51 known mammals.\n",
    "\n",
    "When that new mammal species (with a body weight of 100 kg) is discovered, we still want to make an accurate prediction for its brain weight, but this task might be more difficult, as we don't have all of the data we would ideally like to have.\n",
    "\n",
    "Let's simulate this situation by assigning each of the 51 observations to **either universe 1 or universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set a random seed for reproducibility.\n",
    "np.random.seed(12345)\n",
    "\n",
    "# Randomly assign every observation to either universe 1 or universe 2.\n",
    "mammals['universe'] = np.random.randint(1, 3, len(mammals))\n",
    "mammals.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**Important:** We only live in one of the two universes. Both universes have 51 known mammal species, but each universe knows the brain and body weight for different species.\n",
    "\n",
    "We can now tell Seaborn to create two plots in which the left plot only uses the data from **universe 1** and the right plot only uses the data from **universe 2**:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# col='universe' subsets the data by universe and creates two separate plots.\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe');\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "The line looks pretty similar between the two plots, despite the fact that they used separate samples of data. In both cases, we would predict a brain weight of about 45 g.\n",
    "\n",
    "It's easier to see the degree of similarity by placing them on the same plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hue='universe' subsets the data by universe and creates a single plot.\n",
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe');\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "So, what was the point of this exercise? This was a visual demonstration of a high-bias, low-variance model.\n",
    "\n",
    "- It's **high bias** because it doesn't fit the data particularly well.\n",
    "- It's **low variance** because it doesn't change much depending on which observations happen to be available in that universe."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"lets-try-something-completely-different\"></a>\n",
    "### Let's Try Something Completely Different"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "What would a **low bias, high variance** model look like? Let's try polynomial regression with an eighth-order polynomial."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe', order=8);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- It's **low bias** because the models match the data effectively.\n",
    "- It's **high variance** because the models are widely different, depending on which observations happen to be available in that universe. (For a body weight of 100 kg, the brain weight prediction would be 40 kg in one universe and 0 kg in the other!)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, hue='universe', order=8);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"balancing-bias-and-variance\"></a>\n",
    "## Balancing Bias and Variance\n",
    "Can we find a middle ground?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Perhaps we can create a model that has **less bias than the linear model** and **less variance than the eighth order polynomial**?\n",
    "\n",
    "Let's try a second order polynomial instead:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sns.lmplot(x='body', y='brain', data=mammals, ci=None, col='universe', order=2);\n",
    "plt.xlim(-10, 200);\n",
    "plt.ylim(-10, 250);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This seems better. In both the left and right plots, **it fits the data well, but not too well**.\n",
    "\n",
    "This is the essence of the **bias-variance trade-off**: You are seeking a model that appropriately balances bias and variance and thus will generalize to new data (known as \"out-of-sample\" data)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "We want a model that best balances bias and variance. It\n",
    "should match our training data well (moderate bias) yet be low variance for out-of-sample data (moderate variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- Training error as a function of\n",
    "complexity.\n",
    "- Question: Why do we even\n",
    "care about variance if we\n",
    "know we can generate a\n",
    "more accurate model with\n",
    "higher complexity?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Can we obtain a zero-bias, zero-variance model?\n",
    "\n",
    "No! If there is any noise in the data-generating process, then a zero-variance model would not be learning from the data. Additionally, a model only has zero bias if the true relationship between the target and the features is hard-coded into it. If that were the case, you wouldn't be doing machine learning -- it would be similar to trying to predict today's temperature by using today's temperature!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "<a id=\"train-test-split\"></a>\n",
    "## Train-test-split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For the rest of the lab, we will look at three evaluation procedures for predicting model out-of-sample accuracy:\n",
    "\n",
    "1. **Train on the entire dataset** should never be done to estimate model accuracy on out-of-sample data! After all, training error can be made arbitrarily small or large. You might train on the entire dataset as the very last step when a model is chosen, hoping to make the final model as accurate as possible. Or, you could use this to estimate the degree of overfitting.\n",
    "2. **Train-test-split** is useful if cross-validation is not practical (e.g. it takes too long to train). It is also useful for computing a quick confusion matrix. You could also use this as a final step after the model is finalized (often called evaluating the model against a **validation set**).\n",
    "3. **Cross-validation** is the gold standard for estimating accuracy and comparing accuracy across models.\n",
    "4. **Three-way split** combines cross-validation and the train-test-split. It takes an initial split to be used as a final validation set, then uses cross-validation on the rest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We run into a problem when powerful models can perfectly fit the data on which they are trained. These models are **low bias** and **high variance**. However, we can't observe the variance of a model directly, because we only know how it fits the data we have rather than all potential samples.\n",
    "\n",
    "**Solution:** Create a procedure that **estimates** how well a model is likely to perform on out-of-sample data and use that to choose between models.\n",
    "\n",
    "- Before, we have been splitting the data into a **single training group** and a **single test group**.\n",
    "\n",
    "- Now, to estimate how well the model is likely to perform on out-of-sample data, we will create **many training groups** and **many test groups** then fit **many models**.\n",
    "\n",
    "**Note:** These procedures can be used with **any machine learning model**.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**The Holdout Method: Train/Test Split**\n",
    "- **Training set**: Used to train the classifier.\n",
    "- **Testing set**: Used to estimate the error rate of the trained classifier.\n",
    "- **Advantages**: Fast, simple, computationally inexpensive.\n",
    "- **Disadvantages** Eliminates data, imperfectly splits."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"evaluation-procedure--train-and-test-on-the-entire-dataset-do-not-do-this\"></a>\n",
    "### Evaluation Procedure #1: Train and Test on the Entire Data Set (Do Not Do This)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Train the model on the **entire data set**.\n",
    "2. Test the model on the **same data set** and evaluate how well we did by comparing the **predicted** response values with the **true** response values."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load in the Boston data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.datasets import load_boston\n",
    "\n",
    "boston = load_boston()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(boston.DESCR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create X and y variable to stores the feature matrix and response from the Boston data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a DataFrame for both parts of data; don't forget to assign column names.\n",
    "X = pd.DataFrame(boston.data, columns=boston.feature_names)\n",
    "y = pd.DataFrame(boston.target, columns=['MEDV'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Concatenate y and X, then overwrite the Boston variable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston = pd.concat([y, X], axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Perform basic EDA to make sure the data are in order."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.isnull().sum()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "boston.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Prepare a feature matrix (X) and response (y)  for scikit-learn."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create feature matrix (X)\n",
    "feature_cols = boston.columns.drop(['MEDV'])\n",
    "X = boston[feature_cols]\n",
    "\n",
    "# create response vector (y)\n",
    "y = boston.MEDV"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Import linear regression, instantiate, fit, and preview predictions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import the class.\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Instantiate the model.\n",
    "lr = LinearRegression()\n",
    "\n",
    "# Train the model on the entire data set.\n",
    "lr.fit(X, y)\n",
    "\n",
    "# Predict the response values for the observations in X (\"test the model\").\n",
    "lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Store the predicted response values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "To evaluate a model, we also need an **evaluation metric:**\n",
    "\n",
    "- A numeric calculation used to **quantify** the performance of a model.\n",
    "- The appropriate metric depends on the **goals** of your problem.\n",
    "\n",
    "The most common choices for regression problems are:\n",
    "\n",
    "- **R-squared**: The percentage of variation explained by the model (a \"reward function,\" as higher is better).\n",
    "- **Mean squared error**: The average squared distance between the prediction and the correct answer (a \"loss function,\" as lower is better).\n",
    "\n",
    "In this case, we'll use mean squared error because it is more interpretable in a predictive context."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Compute mean squared error using a function from `metrics`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "\n",
    "print(metrics.mean_squared_error(y, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "This is known as the **training mean squared error** because we are evaluating the model based on the same data we used to train the model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"problems-with-training-and-testing-on-the-same-data\"></a>\n",
    "### Problems With Training and Testing on the Same Data\n",
    "\n",
    "- Our goal is to estimate likely performance of a model on **out-of-sample data**.\n",
    "- But, maximizing the training mean squared error rewards **overly complex models** that won't necessarily generalize.\n",
    "- Unnecessarily complex models **overfit** the training data.\n",
    "    - They will do well when tested using the in-sample data.\n",
    "    - They may do poorly with out-of-sample data.\n",
    "    - They learn the \"noise\" in the data rather than the \"signal.\"\n",
    "    - From Quora: [What is an intuitive explanation of overfitting?](http://www.quora.com/What-is-an-intuitive-explanation-of-overfitting/answer/Jessica-Su)\n",
    "\n",
    "**Thus, the training MSE is not a good estimate of the out-of-sample MSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"evaluation-procedure--traintest-split\"></a>\n",
    "### Evaluation procedure #2: Train/Test Split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Split the data set into two pieces: a **training set** and a **testing set**.\n",
    "2. Train the model on the **training set**.\n",
    "3. Test the model on the **testing set** and evaluate how well we did.\n",
    "\n",
    "Often a good rule-of-thumb is 70% training/30% test, but this can vary based on the size of your dataset. For example, with a small dataset you would need to use as much training data as possible (in return, your test accuracy will be more variable).\n",
    "\n",
    "What does this accomplish?\n",
    "\n",
    "- Models can be trained and tested on **different data** (We treat testing data like out-of-sample data).\n",
    "- Response values are known for the testing set and thus **predictions can be evaluated**.\n",
    "\n",
    "This is known as the **testing mean squared error** because we are evaluating the model on an independent \"test set\" that was not used during model training.\n",
    "\n",
    "**The testing MSE is a better estimate of out-of-sample performance than the training MSE.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Before We Dive Into Train/Test Split, Let's Understand \"Unpacking\" Syntax\n",
    "\n",
    "Unpacking in itself allows us to break down the contents of an object and assign it equally to several variables simultaneously.\n",
    "\n",
    "Let's create a packed object (boxed), then unpack it using a `for` loop."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start with two lists that are related in some manner.\n",
    "package = ['package_1','package_2','package_3','package_4']\n",
    "directions = ['directions_1','directions_2','directions_3','directions_4']\n",
    "\n",
    "# we'll zip them together to form the associate combos\n",
    "# We can then use `for Obj-1, Obj-2 in` to isolate the values we need.\n",
    "for p, d in zip(package, directions):\n",
    "    print('Shipment: {} | Shipment Contents: {}'.format(p,d))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rather than using a `for` loop to unpack an output, we can simply assign the results, assuming we know exactly how many results need to be assigned. We can think of the result of `zip` as comprising four subcomponents; we can use a `for` loop to help us break the subcomponents out OR use the unpacking method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "box1, box2, box3, box4 = zip(package, directions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(box1)\n",
    "print(box3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the case of train/test split, we add an unpackaging assignment to the return value of a function, as exemplified by the code below:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a function that takes an argument to act up. \n",
    "def min_max(nums):\n",
    "    smallest = min(nums)\n",
    "    largest = max(nums)\n",
    "    \n",
    "    # The function returns a list in the order below.\n",
    "    return [smallest, largest, 5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# We can assign the returned list to a single variable,\n",
    "min_and_max = min_max([1, 2, 3])\n",
    "\n",
    "print(min_and_max)\n",
    "print(type(min_and_max))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OR, because we know the list is composed of three elements, \n",
    "# assign each element to its own variable.\n",
    "the_min, the_max, five = min_max([1, 2, 3])\n",
    "\n",
    "print(the_max)\n",
    "print(the_min)\n",
    "print(five)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Understanding the `train_test_split` Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Before splitting\n",
    "print(X.shape)\n",
    "\n",
    "# After splitting\n",
    "print(X_train.shape)\n",
    "print(X_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Recall that (1,) is a tuple. \n",
    "# The trailing comma distinguishes it as being a tuple, not an integer.\n",
    "\n",
    "# Before splitting\n",
    "print(y.shape)\n",
    "\n",
    "# After splitting\n",
    "print(y_train.shape)\n",
    "print(y_test.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![train_test_split](./assets/train_test_split.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "#### Understanding the `random_state` Parameter\n",
    "\n",
    "The `random_state` is a pseudo-random number that allows us to reproduce our results every time we run them. However, it makes it impossible to predict what are exact results will be if we chose a new `random_state`.\n",
    "\n",
    "`random_state` is very useful for testing that your model was made correctly since it provides you with the same split each time. However, make sure you remove it if you are testing for model variability!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITHOUT a random_state parameter:\n",
    "#  (If you run this code several times, you get different results!)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y)\n",
    "\n",
    "# Print the first element of each object.\n",
    "print(X_train.head(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# WITH a random_state parameter:\n",
    "#  (Same split every time! Note you can change the random state to any integer.)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=1)\n",
    "\n",
    "# Print the first element of each object.\n",
    "print(X_train.head(1))\n",
    "print(X_test.head(1))\n",
    "print(y_train.head(1))\n",
    "print(y_test.head(1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Introduce Patsy\n",
    "\n",
    "We will make one more modification. Patsy is a library that allows you to quickly perform simple data transformations in a style similar to R.\n",
    "\n",
    "Rather than manually creating X and y, we will use the `.dmatricies()` function from Patsy to create the matricies and explore the effect of changing features on training and testing error."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import patsy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1: Split X and y into training and testing sets (using `random_state` for reproducibility)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y, X = patsy.dmatrices(\"MEDV ~ AGE + RM\", data=boston, return_type=\"dataframe\")\n",
    "\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, random_state=99)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2: Train the model on the training set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "lr = LinearRegression()\n",
    "lr.fit(X_train, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3: Test the model on the testing set and check the accuracy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = lr.predict(X_test)\n",
    "\n",
    "print(metrics.mean_squared_error(y_train, lr.predict(X_train)))\n",
    "print(metrics.mean_squared_error(y_test, y_pred))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![Bias-variance tradeoff](./assets/bias_variance.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Go back to Step 1 and try adding new variables and transformations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- **Training error**: Decreases as model complexity increases (lower value of k).\n",
    "- **Testing error**: Is minimized at the optimum model complexity."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"comparing-test-performance-with-a-null-baseline\"></a>\n",
    "### Comparing Test Performance With a Null Baseline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "When interpreting the predictive power of a model, it's best to compare it to a baseline using a dummy model, sometimes called a ZeroR model or a baseline model. A dummy model is simply using the mean, median, or most common value as the prediction. This forms a benchmark to compare your model against and becomes especially important in classification where your null accuracy might be 95 percent.\n",
    "\n",
    "For example, suppose your dataset is **imbalanced** -- it contains 99% one class and 1% the other class. Then, your baseline accuracy (always guessing the first class) would be 99%. So, if your model is less than 99% accurate, you know it is worse than the baseline. Imbalanced datasets generally must be trained differently (with less of a focus on accuracy) because of this.\n",
    "\n",
    "You can alternatively use simple models to achieve baseline results, for example nearest neighbors or a basic unigram bag of words for text data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Examine the baseline mean squared error using a null model.\n",
    "How does this compare to what we achieved with linear regression. Is our model making an actual improvement?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Use .apply() to broadcast a mean for every prediction.\n",
    "print(metrics.mean_squared_error(y_test, y_test.apply(np.mean, broadcast=True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"k-folds-cross-validation\"></a>\n",
    "## K-Folds Cross-Validation\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Train/test split provides us with helpful tool, but it's a shame that we are tossing out a large chunk of our data for testing purposes.\n",
    "\n",
    "**How can we use the maximum amount of our data points while still ensuring model integrity?**\n",
    "\n",
    "1. Split our data into a number of different pieces (folds).\n",
    "2. Train using `k-1` folds for training and a different fold for testing.\n",
    "3. Average our model against EACH of those iterations.\n",
    "4. Choose our model and TEST it against the final fold.\n",
    "5. Average all test accuracies to get the estimated out-of-sample accuracy.\n",
    "\n",
    "Although this may sound complicated, we are just training the model on k separate train-test-splits, then taking the average of the resulting test accuracies!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/cross_validation_diagram.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"leave-one-out-cross-validation\"></a>\n",
    "### Leave-One-Out Cross-Validation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "A special case of k-fold cross-validation is leave-one-out cross-validation. Rather than taking 5–10 folds, we take a fold of size `n-1` and leave one observation to test. \n",
    "\n",
    "Typically, 5–10 fold cross-validaiton is recommended."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"intro-to-cross-validation-with-the-boston-data\"></a>\n",
    "### Intro to Cross-Validation With the Boston Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Create a cross-valiation with five folds."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import model_selection"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "kf = model_selection.KFold(n_splits=5, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mse_values = []\n",
    "scores = []\n",
    "n = 0\n",
    "\n",
    "print(\"~~~~ CROSS VALIDATION each fold ~~~~\")\n",
    "for train_index, test_index in kf.split(X, y):\n",
    "    lr = LinearRegression().fit(X.iloc[train_index], y.iloc[train_index])\n",
    "    \n",
    "    mse_values.append(metrics.mean_squared_error(y.iloc[test_index], lr.predict(X.iloc[test_index])))\n",
    "    scores.append(lr.score(X, y))\n",
    "    \n",
    "    n += 1\n",
    "    \n",
    "    print('Model {}'.format(n))\n",
    "    print('MSE: {}'.format(mse_values[n-1]))\n",
    "    print('R2: {}\\n'.format(scores[n-1]))\n",
    "\n",
    "\n",
    "print(\"~~~~ SUMMARY OF CROSS VALIDATION ~~~~\")\n",
    "print('Mean of MSE for all folds: {}'.format(np.mean(mse_values)))\n",
    "print('Mean of R2 for all folds: {}'.format(np.mean(scores)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Note the results will vary each run since we take a different\n",
    "#   subset of the data each time (since shuffle=True)\n",
    "kf = model_selection.KFold(n_splits=5, shuffle=True)\n",
    "\n",
    "print(np.mean(-cross_val_score(lr, X, y, cv=kf, scoring='neg_mean_squared_error')))\n",
    "print(np.mean(cross_val_score(lr, X, y, cv=kf)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "While the cross-validated approach here generated more overall error, which of the two approaches would predict new data more accurately — the single model or the cross-validated, averaged one? Why?\n",
    "\n",
    "\n",
    "\n",
    "**Answer**: \n",
    "\n",
    "...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a id=\"three-way-data-split\"></a>\n",
    "## Three-Way Data Split\n",
    "---\n",
    "\n",
    "The most common workflow is actually a combination of train/test split and cross-validation. We take a train/test split on our data right away and try not spend a lot of time using the testing data set. Instead, we take our training data and tune our models using cross-validation. When we think we are done, we do one last test on the testing data to make sure we haven't accidently overfit to our training data.\n",
    "\n",
    "**If you tune hyperparameters via cross-validation, you should never use cross-validation on the same dataset to estimate OOS accuracy!** Using cross-validation in this way, the entire dataset was used to tune hyperparameters. So, this invalidates our condition above -- where we assumed the test set is a pretend \"out-of-sample\" dataset that was not used to train our model! So, we would expect the accuracy on this test set to be artificially inflated as compared to actual \"out-of-sample\" data.\n",
    "\n",
    "Even with good evaluation procedures, it is incredible easy to overfit our models by including features that will not be available during production or leak information about our testing data in other ways."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![](./assets/Train-Test-Split-CV.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "- If model selection and true error estimates are to be computed simultaneously, three disjointed data sets are best.\n",
    "    - **Training set**: A set of examples used for learning – what parameters of the classifier?\n",
    "    - **Validation set**: A set of examples used to tune the parameters of the classifier.\n",
    "    - **Testing set**: A set of examples used ONLY to assess the performance of the fully trained classifier.\n",
    "- Validation and testing must be separate data sets. Once you have the final model set, you cannot do any additional tuning after testing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "1. Divide data into training, validation, and testing sets.\n",
    "2. Select architecture (model type) and training parameters (k).\n",
    "3. Train the model using the training set.\n",
    "4. Evaluate the model using the training set.\n",
    "5. Repeat 2–4 times, selecting different architectures (models) and tuning parameters.\n",
    "6. Select the best model.\n",
    "7. Assess the model with the final testing set."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"additional-resources\"></a>\n",
    "<a id=\"additional-resources\"></a>\n",
    "### Additional Resources\n",
    "- [Bias Variance](http://scott.fortmann-roe.com/docs/BiasVariance.html)\n",
    "- University of Washington [slides](https://courses.cs.washington.edu/courses/cse546/12wi/slides/)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id=\"summary\"></a>\n",
    "### Summary\n",
    "\n",
    "In this lab, we compared four methods of estimating model accuracy on out-of-sample data. Throughout your regular data science work, you will likely use all four at some point:\n",
    "\n",
    "1. **Train on the entire dataset**\n",
    "2. **Train-test-split**\n",
    "3. **Cross-validation**\n",
    "4. **Three-way split**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
